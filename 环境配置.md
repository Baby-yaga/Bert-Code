# 在AutoDL配置Bert模型
## Step 1 访问AutoDL网站(https://autodl.com/)创建实例

## Step 2 打开Jupyter终端运行如下代码
```python
!pip install pandas
!pip install numpy
!pip install scikit-learn
!pip install transformers
!pip install sentence_transformers

## 择其一+安装完成后会显示模型安装路径
!HF_ENDPOINT=https://hf-mirror.com huggingface-cli download --resume-download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
!HF_ENDPOINT=https://hf-mirror.com huggingface-cli download bert-base-chinese
```
## Step 3 构建预训练数据集
详情见Code

# 在AutoDL配置Bert模型
## 访问AutoDL网站(https://autodl.com/)创建实例

## Step 2 打开Jupyter终端运行如下代码
```python
python -m pip install --upgrade pip

pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
pip install fastapi==0.104.1
pip install uvicorn==0.24.0.post1
pip install requests==2.25.1
pip install modelscope==1.11.0
pip install transformers==4.37.0
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
pip install transformers_stream_generator==0.0.4
```

# Step 3 下载模型

在 /root/autodl-tmp 路径下新建 model_download.py 文件并在其中输入以下代码
```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('Qwen/Qwen2.5-7B-Instruct', cache_dir='/root/autodl-tmp', revision='master')
```
其中，`Qwen/Qwen2.5-7B-Instruct`可以自行调整。

# Step 4 调用模型（示例代码）
## 配置环境
```python
from modelscope import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-7B-Instruct" # 同上

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
```
## 配置任务（文本分类为例）
```python
prompt = "在气候政策愈发不确定性的情况下，提高存货周转水平。"
messages = [
    {"role": "system", "content": "你是一个分析语句中包含气候政策不确定性内容的机器，请为我判断这句话是否体现了气候政策的不确定性，然后按照相关的概率（0~1）为我范围一个包含四位小数的数值。返回的格式是一个字符串，开头是'概率值：'，冒号后面是具体的概率值，不需要返回其他信息。"},
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
```
## 返回结果
```python
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
probs = eval(str.split(response, sep = "：")[1])
print(probs)
```
